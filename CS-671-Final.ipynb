{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milesfking/2020-presidential-election-model/blob/main/CS-671-Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFyA3g7VMW-K"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sUPOaBNaJ4yU"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import argparse\n",
        "import os, sys\n",
        "import time\n",
        "import datetime\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePvLLYjrKP9z"
      },
      "source": [
        "# Load in Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "MQHd3af9KQ6G"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"kaggle_train.csv\")\n",
        "test_df = pd.read_csv(\"kaggle_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKEgJr5OM0ti"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOpLk4boNB40"
      },
      "source": [
        "## Dropping columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wSdSHNlM3qS"
      },
      "source": [
        "We want to begin by dropping any features that are not relevant for prediction, including metadata like website URLs, index, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "0lquoCvELbnt"
      },
      "outputs": [],
      "source": [
        "model_features = [\n",
        "       'host_id', 'host_since', 'host_is_superhost',\n",
        "       'host_listings_count', 'host_total_listings_count',\n",
        "       'host_verifications', 'host_has_profile_pic', 'host_identity_verified',\n",
        "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
        "       'longitude', 'property_type', 'room_type', 'accommodates',\n",
        "       'bathrooms_text', 'beds', 'amenities', 'minimum_nights',\n",
        "       'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights',\n",
        "       'minimum_maximum_nights', 'maximum_maximum_nights',\n",
        "       'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability',\n",
        "       'availability_30', 'availability_60', 'availability_90',\n",
        "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
        "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'instant_bookable',\n",
        "       'calculated_host_listings_count',\n",
        "       'calculated_host_listings_count_entire_homes',\n",
        "       'calculated_host_listings_count_private_rooms',\n",
        "       'calculated_host_listings_count_shared_rooms']\n",
        "\n",
        "X_train = train_df[model_features]\n",
        "y_train = train_df['price']\n",
        "\n",
        "X_test = test_df[model_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6l6UVOh8075"
      },
      "source": [
        "## Drop missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "HKnjw9-4-Rgs"
      },
      "outputs": [],
      "source": [
        "# Drop rows with NaN values in X_train\n",
        "X_train_clean = X_train.dropna()\n",
        "\n",
        "# Get the indices of the dropped rows\n",
        "dropped_indices = X_train.index.difference(X_train_clean.index)\n",
        "\n",
        "# Drop corresponding rows in y_train\n",
        "y_train_clean = y_train.drop(dropped_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering"
      ],
      "metadata": {
        "id": "OjARx0k2kHOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Convert 'host_since' to datetime type\n",
        "X_train_clean.loc[:, 'host_since'] = pd.to_datetime(X_train_clean['host_since'], format='%Y-%m-%d')\n",
        "X_test.loc[:, 'host_since'] = pd.to_datetime(X_train_clean['host_since'], format='%Y-%m-%d')\n",
        "\n",
        "# Use the current date or the maximum date from your dataset as the reference date\n",
        "current_date = X_train_clean['host_since'].max()\n",
        "\n",
        "# Calculate the number of years as a host\n",
        "X_train_clean.loc[:, 'years_as_host'] = (current_date - X_train_clean['host_since']).dt.days / 365.25\n",
        "X_test.loc[:, 'years_as_host'] = (current_date - X_train_clean['host_since']).dt.days / 365.25\n",
        "\n",
        "# Optionally, drop the original 'host_since' column\n",
        "X_train_clean = X_train_clean.drop(columns=['host_since'])\n",
        "X_test = X_test.drop(columns=['host_since'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaJ0ppgDkKPk",
        "outputId": "027dea86-6faf-459d-fcee-adeadc22720c"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-772dd9604f32>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train_clean.loc[:, 'host_since'] = pd.to_datetime(X_train_clean['host_since'], format='%Y-%m-%d')\n",
            "<ipython-input-102-772dd9604f32>:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  X_train_clean.loc[:, 'host_since'] = pd.to_datetime(X_train_clean['host_since'], format='%Y-%m-%d')\n",
            "<ipython-input-102-772dd9604f32>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test.loc[:, 'host_since'] = pd.to_datetime(X_train_clean['host_since'], format='%Y-%m-%d')\n",
            "<ipython-input-102-772dd9604f32>:5: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  X_test.loc[:, 'host_since'] = pd.to_datetime(X_train_clean['host_since'], format='%Y-%m-%d')\n",
            "<ipython-input-102-772dd9604f32>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train_clean.loc[:, 'years_as_host'] = (current_date - X_train_clean['host_since']).dt.days / 365.25\n",
            "<ipython-input-102-772dd9604f32>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test.loc[:, 'years_as_host'] = (current_date - X_train_clean['host_since']).dt.days / 365.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WutAlB8749p"
      },
      "source": [
        "## Encode and normalize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "Vx3fkMuTSNkg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Separate features into numeric and categorical\n",
        "numeric_columns = X_train_clean.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_columns = X_train_clean.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define a function to replace less frequent categories with 'Other'\n",
        "def replace_rare_categories(data, column, threshold=0.02):\n",
        "    series = pd.value_counts(data[column])\n",
        "    mask = series / series.sum() <= threshold\n",
        "    rare_categories = series[mask].index\n",
        "    data.loc[data[column].isin(rare_categories), column] = 'Other'\n",
        "    return data\n",
        "\n",
        "# Apply the function to each categorical column\n",
        "for col in categorical_columns:\n",
        "    X_train_clean = replace_rare_categories(X_train_clean, col)\n",
        "\n",
        "# Rest of your code for preprocessing remains the same\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_columns),\n",
        "        ('cat', categorical_transformer, categorical_columns)\n",
        "    ])\n",
        "\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train_clean)\n",
        "y_train_preprocessed = y_train_clean.to_numpy().astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i97YH6-8hYb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_all_seeds(RANDOM_SEED):\n",
        "    random.seed(RANDOM_SEED)     # python random generator\n",
        "    np.random.seed(RANDOM_SEED)  # numpy random generator\n",
        "\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_all_seeds(42)"
      ],
      "metadata": {
        "id": "Ttl3afct-4n9"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "We1SxmRu_yf3"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple fully connected network for regression\n",
        "class OrdinalClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(OrdinalClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 6)  # 6 output units for 6 categories\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No softmax here, it's included in nn.CrossEntropyLoss\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZjO4_GCX-6rt"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data (assuming X_train_preprocessed and y_train_preprocessed are numpy arrays)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_preprocessed, y_train_preprocessed, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "mygzelDe_E3t"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert your data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize the network and optimizer\n",
        "INITIAL_LR = 0.01\n",
        "input_size = X_train.shape[1]\n",
        "net = OrdinalClassifier(input_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=0.9)"
      ],
      "metadata": {
        "id": "mYDoQuiS_Jm1"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "CHECKPOINT_FOLDER = \"./saved_model\"\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Training Loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for i in range(EPOCHS):\n",
        "    net.train()\n",
        "\n",
        "    # Record training loss\n",
        "    train_loss = 0\n",
        "\n",
        "    # Looping through training loader\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # Send input and target to device\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        outputs = net(inputs)  # Forward pass: compute the model output\n",
        "        loss = criterion(outputs, targets)  # Compute loss\n",
        "        loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        optimizer.step()  # Perform a single optimization step\n",
        "\n",
        "        train_loss += loss.item()  # Sum up batch loss\n",
        "\n",
        "    # calculate average training loss\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Evaluate the validation set performance\n",
        "    net.eval()\n",
        "\n",
        "    # Record validation loss\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Send input and target to device\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    print(f'Epoch {i}: Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Save the model if validation loss has decreased\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "            os.makedirs(CHECKPOINT_FOLDER)\n",
        "        torch.save(net.state_dict(), os.path.join(CHECKPOINT_FOLDER, 'best_model.bin'))\n",
        "\n",
        "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD28gfnU_NO3",
        "outputId": "a6569b80-b850-457b-8418-c30b66599fd9"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss: 1.4347, Validation Loss: 1.2565, Validation Accuracy: 0.4792\n",
            "Epoch 1: Train Loss: 1.2293, Validation Loss: 1.2423, Validation Accuracy: 0.4795\n",
            "Epoch 2: Train Loss: 1.1780, Validation Loss: 1.1651, Validation Accuracy: 0.4936\n",
            "Epoch 3: Train Loss: 1.1527, Validation Loss: 1.1876, Validation Accuracy: 0.4970\n",
            "Epoch 4: Train Loss: 1.1367, Validation Loss: 1.1550, Validation Accuracy: 0.5134\n",
            "Epoch 5: Train Loss: 1.1197, Validation Loss: 1.1251, Validation Accuracy: 0.5235\n",
            "Epoch 6: Train Loss: 1.1083, Validation Loss: 1.1369, Validation Accuracy: 0.5185\n",
            "Epoch 7: Train Loss: 1.0993, Validation Loss: 1.1545, Validation Accuracy: 0.5131\n",
            "Epoch 8: Train Loss: 1.0882, Validation Loss: 1.1312, Validation Accuracy: 0.5178\n",
            "Epoch 9: Train Loss: 1.0748, Validation Loss: 1.1291, Validation Accuracy: 0.5161\n",
            "Epoch 10: Train Loss: 1.0671, Validation Loss: 1.1330, Validation Accuracy: 0.5175\n",
            "Epoch 11: Train Loss: 1.0547, Validation Loss: 1.1158, Validation Accuracy: 0.5279\n",
            "Epoch 12: Train Loss: 1.0487, Validation Loss: 1.1225, Validation Accuracy: 0.5215\n",
            "Epoch 13: Train Loss: 1.0408, Validation Loss: 1.1306, Validation Accuracy: 0.5232\n",
            "Epoch 14: Train Loss: 1.0267, Validation Loss: 1.1171, Validation Accuracy: 0.5252\n",
            "Epoch 15: Train Loss: 1.0222, Validation Loss: 1.1355, Validation Accuracy: 0.5265\n",
            "Epoch 16: Train Loss: 1.0093, Validation Loss: 1.1327, Validation Accuracy: 0.5272\n",
            "Epoch 17: Train Loss: 1.0119, Validation Loss: 1.1195, Validation Accuracy: 0.5201\n",
            "Epoch 18: Train Loss: 0.9944, Validation Loss: 1.1127, Validation Accuracy: 0.5349\n",
            "Epoch 19: Train Loss: 0.9915, Validation Loss: 1.1345, Validation Accuracy: 0.5262\n",
            "Epoch 20: Train Loss: 0.9745, Validation Loss: 1.1217, Validation Accuracy: 0.5252\n",
            "Epoch 21: Train Loss: 0.9740, Validation Loss: 1.1259, Validation Accuracy: 0.5232\n",
            "Epoch 22: Train Loss: 0.9601, Validation Loss: 1.1221, Validation Accuracy: 0.5336\n",
            "Epoch 23: Train Loss: 0.9548, Validation Loss: 1.1368, Validation Accuracy: 0.5245\n",
            "Epoch 24: Train Loss: 0.9450, Validation Loss: 1.1219, Validation Accuracy: 0.5309\n",
            "Epoch 25: Train Loss: 0.9355, Validation Loss: 1.1493, Validation Accuracy: 0.5285\n",
            "Epoch 26: Train Loss: 0.9287, Validation Loss: 1.1357, Validation Accuracy: 0.5255\n",
            "Epoch 27: Train Loss: 0.9204, Validation Loss: 1.1481, Validation Accuracy: 0.5379\n",
            "Epoch 28: Train Loss: 0.9075, Validation Loss: 1.1755, Validation Accuracy: 0.5201\n",
            "Epoch 29: Train Loss: 0.9037, Validation Loss: 1.1552, Validation Accuracy: 0.5269\n",
            "Best Validation Loss: 1.1127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "Mp96w2hbqHHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform X_test with the same preprocessor\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Convert to DataFrame (optional, for handling column alignment)\n",
        "X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=preprocessor.get_feature_names_out())\n",
        "X_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, columns=preprocessor.get_feature_names_out())\n",
        "\n",
        "# Add missing columns in X_test with zeros\n",
        "missing_cols = set(X_train_preprocessed_df.columns) - set(X_test_preprocessed_df.columns)\n",
        "for c in missing_cols:\n",
        "    X_test_preprocessed_df[c] = 0\n",
        "\n",
        "# Ensure the order of columns is the same\n",
        "X_test_preprocessed_df = X_test_preprocessed_df[X_train_preprocessed_df.columns]\n",
        "X_test_preprocessed = X_test_preprocessed_df.to_numpy()"
      ],
      "metadata": {
        "id": "egHyYPYoqGCb"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "best_model_path = os.path.join(CHECKPOINT_FOLDER, 'best_model.bin')\n",
        "state_dict = torch.load(best_model_path)\n",
        "net.load_state_dict(state_dict)\n",
        "\n",
        "# Prepare your test data\n",
        "# Assuming X_test_preprocessed is your test data and already a numpy array\n",
        "X_test_tensor = torch.tensor(X_test_preprocessed, dtype=torch.float32).to(device)\n",
        "\n",
        "# Create a DataLoader for the test set if necessary\n",
        "# Assuming batch size of 32 for consistency\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Make predictions\n",
        "net.eval()  # Set the model to evaluation mode\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        inputs = inputs[0].to(device)  # Move inputs to the device\n",
        "        outputs = net(inputs)\n",
        "        _, predicted_classes = torch.max(outputs, 1)  # Get the predicted classes\n",
        "        predictions.extend(predicted_classes.cpu().numpy())\n",
        "\n",
        "# Convert predictions to a numpy array\n",
        "predictions = np.array(predictions)"
      ],
      "metadata": {
        "id": "SNNwKWxoAjPJ"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame\n",
        "df_predictions = pd.DataFrame({\n",
        "    'price': predictions\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM5mVe9QAoiy",
        "outputId": "120eeb0a-4cd4-4c2b-f84b-2f292cf5ad6e"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      price\n",
            "0         3\n",
            "1         3\n",
            "2         3\n",
            "3         3\n",
            "4         3\n",
            "...     ...\n",
            "6286      5\n",
            "6287      4\n",
            "6288      1\n",
            "6289      0\n",
            "6290      3\n",
            "\n",
            "[6291 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_predictions.to_csv(\"submission1.csv\", index=True, index_label='id')"
      ],
      "metadata": {
        "id": "jImeEL0WEwhi"
      },
      "execution_count": 204,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr9+3EJN8y7EsvKi+sALM7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}